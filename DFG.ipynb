{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "#coding=utf-8\n",
    "import setproctitle\n",
    "setproctitle.setproctitle('BeijingFlow@shaoerzhuo')\n",
    "\n",
    "import os, tqdm, torch, sys\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils.datasets import DFGDataset, collate_fn\n",
    "import utils.metrics as metrics\n",
    "from model.correlation import get_CORR_numpy, get_CORR_numpy2, get_CORR_torch, get_CORR_torch2\n",
    "\n",
    "class config:\n",
    "    cuda_num = 6\n",
    "    cityname = 'shanghai'\n",
    "    io = 'outflow'\n",
    "    time_slice = 48\n",
    "    \n",
    "    batch_size = 2**13\n",
    "    interval = 50\n",
    "\n",
    "checkin_cate = 14\n",
    "poi_cate = 14\n",
    "\n",
    "dataset_config = {\n",
    "    'cityname' : config.cityname,\n",
    "    'max_value' : {'beijing':{'inflow':1587.8500, 'outflow':1929.5500}, 'shanghai':{'inflow':8179.6667, 'outflow':9417.3333}}[config.cityname][config.io],\n",
    "    'beta' : 1,\n",
    "    'dataset_path' : os.path.join('/data2/shaoerzhuo/DeepFlowGen/Dataset', config.cityname, 'dataset'),\n",
    "    'i/o' : config.io,\n",
    "    'poi_cate' : poi_cate\n",
    "}\n",
    "\n",
    "print(dataset_config)\n",
    "\n",
    "model_config = {\n",
    "    'len_time_vec':64,\n",
    "    'epoch_num' : {'beijing':{'inflow':5000, 'outflow':5000}, 'shanghai':{'inflow':5000, 'outflow':5000}}[config.cityname][config.io],\n",
    "    'hidden_dims':{'beijing':256, 'shanghai':256}[config.cityname],\n",
    "    'alpha':{'beijing':{'inflow':1e-3, 'outflow':3e-4}, 'shanghai':{'inflow':3e-4, 'outflow':1e-4}}[config.cityname][config.io],\n",
    "    'lr':{'beijing':1e-3, 'shanghai':1e-3}[config.cityname],\n",
    "}\n",
    "\n",
    "print(model_config)\n",
    "\n",
    "train_dataset = DFGDataset(dataset_config, 'train')\n",
    "valid_dataset = DFGDataset(dataset_config, 'valid')\n",
    "test_dataset = DFGDataset(dataset_config, 'test')\n",
    "\n",
    "num_workers = 10\n",
    "train_loader = DataLoader(train_dataset, num_workers=num_workers, batch_size=config.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, num_workers=num_workers, batch_size=config.batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "nmf = NMF(14)\n",
    "W = nmf.fit_transform(train_dataset.flow)\n",
    "H = nmf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iacf = np.zeros([train_dataset.num_regions,48,14])\n",
    "for iter in range(14):\n",
    "    for iter2 in range(train_dataset.num_regions):\n",
    "        iacf[iter2, :, iter] = W[iter2, iter] * H[iter]\n",
    "\n",
    "pred_checkin = np.zeros([train_dataset.num_regions,48,14])\n",
    "for iter in range(48):\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(iacf[:, iter], train_dataset.checkin[:, iter])\n",
    "    pred_checkin[:, iter] = lr.predict(iacf[:, iter])\n",
    "print(get_CORR_numpy2(pred_checkin, train_dataset.checkin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossLayer(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(CrossLayer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.weight = nn.Parameter(torch.zeros([self.input_dim]))\n",
    "        self.bias = nn.Parameter(torch.zeros([self.input_dim]))\n",
    "        torch.nn.init.normal_(self.weight.data, 0.0, 0.1)\n",
    "        torch.nn.init.normal_(self.bias.data, 0.0, 0.1)\n",
    "        \n",
    "    def forward(self, x0, x):\n",
    "        # [B, C]\n",
    "        return x0 * torch.sum(x * self.weight, dim=1, keepdim=True) + self.bias + x\n",
    "        #return torch.matmul(torch.bmm(torch.unsqueeze(x0, 2), torch.unsqueeze(xl, 1)), self.kernal) + self.bias + xl\n",
    "\n",
    "\n",
    "class DeepFlowGen(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepFlowGen, self).__init__()\n",
    "        self.hidden_dims = model_config['hidden_dims']\n",
    "        self.time_dims = model_config['len_time_vec']\n",
    "        self.cuda_num = config.cuda_num\n",
    "        \n",
    "        self.time_embedding_layer = nn.Embedding(48, self.time_dims)\n",
    "        self.feat = nn.Sequential(nn.Linear(2 * poi_cate, self.hidden_dims), nn.Sigmoid())\n",
    "        \n",
    "        self.fcl0 = CrossLayer(self.hidden_dims + self.time_dims)\n",
    "        self.fcl1 = CrossLayer(self.hidden_dims + self.time_dims)\n",
    "        self.fcl2 = CrossLayer(self.hidden_dims + self.time_dims)\n",
    "        self.fcl3 = CrossLayer(self.hidden_dims + self.time_dims)\n",
    "        self.fcl4 = CrossLayer(self.hidden_dims + self.time_dims)\n",
    "        \n",
    "        self.fc0 = nn.Sequential(nn.Linear(self.hidden_dims + self.time_dims, self.hidden_dims), nn.Sigmoid())\n",
    "        \n",
    "        self.fc1 = nn.Sequential(nn.Linear(self.hidden_dims, self.hidden_dims), nn.Sigmoid())\n",
    "        self.fc2 = nn.Sequential(nn.Linear(self.hidden_dims, self.hidden_dims), nn.Sigmoid())\n",
    "        self.fc3 = nn.Sequential(nn.Linear(self.hidden_dims, self.hidden_dims), nn.Sigmoid())\n",
    "        self.fc4 = nn.Sequential(nn.Linear(self.hidden_dims, self.hidden_dims), nn.Sigmoid())\n",
    "        self.fc5 = nn.Sequential(nn.Linear(self.hidden_dims, self.hidden_dims), nn.Sigmoid())\n",
    "        self.fc6 = nn.Sequential(nn.Linear(self.hidden_dims, self.hidden_dims), nn.Sigmoid())\n",
    "            \n",
    "        self.fc7 = nn.Sequential(nn.Linear(self.hidden_dims, checkin_cate))\n",
    "        self.checkin_rate_embed_1 = nn.Embedding(48, checkin_cate)\n",
    "        self.checkin_rate_embed_2 = nn.Embedding(48, checkin_cate)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            classname = m.__class__.__name__\n",
    "            if classname.find('Linear') != -1:\n",
    "                torch.nn.init.normal_(m.weight.data, 0.0, 0.1)\n",
    "                torch.nn.init.normal_(m.bias.data, 0.0, 0.1)\n",
    "            elif classname.find('Embedding') != -1:\n",
    "                torch.nn.init.normal_(m.weight.data, 0.0, 0.1)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        poi = batch['poi'].cuda(self.cuda_num)\n",
    "        t = batch['t'].cuda(self.cuda_num)\n",
    "        \n",
    "        x0 = torch.cat([self.feat(poi), self.time_embedding_layer(t)[:, 0]], dim=1)\n",
    "        x = x0\n",
    "        #x = self.fcl0(x0, x)\n",
    "        #x = self.fcl1(x0, x)\n",
    "        #x = self.fcl2(x0, x)\n",
    "        #x = self.fcl3(x0, x)\n",
    "        #x = self.fcl4(x0, x)\n",
    "        \n",
    "        h = self.fc0(x)\n",
    "        h = self.fc2(self.fc1(h) + h)\n",
    "        h = self.fc4(self.fc3(h) + h)\n",
    "        #h = self.fc6(self.fc5(h) + h)\n",
    "        \n",
    "        IACF = self.fc7(h)\n",
    "        checkin_rate = torch.exp(self.checkin_rate_embed_1(t)[:, 0])\n",
    "        checkin = IACF * checkin_rate\n",
    "        \n",
    "        return {'total_crowd_flow' : IACF.mean(-1, keepdim=True), 'IACF': IACF, 'checkin':checkin, 'checkin_rate' : checkin_rate}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepFlowGen().cuda(config.cuda_num)\n",
    "model.criterion = nn.MSELoss().cuda(config.cuda_num)\n",
    "regularizetion_loss = 0\n",
    "model.optimizer = optim.Adam(model.parameters(), lr=model_config['lr'], betas=(0.9, 0.999), eps=1e-8)\n",
    "model.lr_scheduler = optim.lr_scheduler.ExponentialLR(model.optimizer, 0.5**(1/2000))\n",
    "\n",
    "valid_list = [np.inf]\n",
    "min_string = ''\n",
    "for epoch in tqdm.tqdm(range(model_config['epoch_num']), ncols=70, ascii=True):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        model.optimizer.zero_grad()\n",
    "        return_dict = model(batch)\n",
    "        p_total = return_dict['total_crowd_flow']\n",
    "        t_total = batch['flow'].cuda(config.cuda_num)\n",
    "        \n",
    "        p_checkin = return_dict['checkin']\n",
    "        t_checkin = batch['checkin'].cuda(config.cuda_num)\n",
    "        \n",
    "        checkin_rate = return_dict['checkin_rate']\n",
    "        loss_1 = model.criterion(p_total, t_total)\n",
    "        loss_2 = - model_config['alpha'] * get_CORR_torch2(p_checkin, t_checkin)\n",
    "        #loss_3 = - torch.sum(checkin_rate*torch.log(checkin_rate), dim=-1).mean()\n",
    "        loss = loss_1 + loss_2# + loss_3\n",
    "        loss.backward(retain_graph=True)\n",
    "        model.optimizer.step()\n",
    "    model.lr_scheduler.step()\n",
    "    if epoch % config.interval == config.interval-1:\n",
    "        model.eval()\n",
    "        print('Loss 1 = \\t{0:.4f}\\tLoss 2 = \\t{1:.4f}\\t'.format(loss_1, loss_2))\n",
    "        pred_total = []\n",
    "        target_total = []\n",
    "        pred_checkin = []\n",
    "        target_checkin = []\n",
    "        for batch in train_loader:\n",
    "            pred_total.append(model(batch)['total_crowd_flow'].detach().cpu().numpy())\n",
    "            target_total.append(batch['flow'].numpy())\n",
    "            pred_checkin.append(model(batch)['checkin'].detach().cpu().numpy())\n",
    "            target_checkin.append(batch['checkin'].numpy())\n",
    "        pred_total = np.concatenate(pred_total, axis=0)\n",
    "        target_total = np.concatenate(target_total, axis=0)\n",
    "        pred_checkin = np.concatenate(pred_checkin, axis=0)\n",
    "        target_checkin = np.concatenate(target_checkin, axis=0)\n",
    "        \n",
    "        MAE = metrics.get_MAE(pred_total, target_total) * dataset_config['max_value']\n",
    "        RMSE = metrics.get_RMSE(pred_total, target_total) * dataset_config['max_value']\n",
    "        NRMSE = metrics.get_NRMSE(pred_total, target_total)\n",
    "        CORR = get_CORR_numpy2(pred_checkin, target_checkin)\n",
    "        print('Epoch={0}\\tMAE=\\t{1:.6f}\\tRMSE=\\t{2:.6f}\\tNRMSE=\\t{3:.6f}\\tCORR=\\t{4:.6f}'.format(epoch, MAE, RMSE, NRMSE, CORR))\n",
    "        \n",
    "        \n",
    "        pred_total = []\n",
    "        target_total = []\n",
    "        pred_checkin = []\n",
    "        target_checkin = []\n",
    "        for batch in valid_loader:\n",
    "            pred_total.append(model(batch)['total_crowd_flow'].detach().cpu().numpy())\n",
    "            target_total.append(batch['flow'].numpy())\n",
    "            pred_checkin.append(model(batch)['checkin'].detach().cpu().numpy())\n",
    "            target_checkin.append(batch['checkin'].numpy())\n",
    "        pred_total = np.concatenate(pred_total, axis=0)\n",
    "        target_total = np.concatenate(target_total, axis=0)\n",
    "        pred_checkin = np.concatenate(pred_checkin, axis=0)\n",
    "        target_checkin = np.concatenate(target_checkin, axis=0)\n",
    "        \n",
    "        MAE = metrics.get_MAE(pred_total, target_total) * dataset_config['max_value']\n",
    "        RMSE = metrics.get_RMSE(pred_total, target_total) * dataset_config['max_value']\n",
    "        NRMSE = metrics.get_NRMSE(pred_total, target_total)\n",
    "        CORR = get_CORR_numpy2(pred_checkin, target_checkin)\n",
    "        print('Epoch={0}\\tMAE=\\t{1:.6f}\\tRMSE=\\t{2:.6f}\\tNRMSE=\\t{3:.6f}\\tCORR=\\t{4:.6f}'.format(epoch, MAE, RMSE, NRMSE, CORR))\n",
    "        \n",
    "torch.save(model, 'checkpoints/DFG-DCN-{0}-{1}.pkl'.format(config.cityname, config.io))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_IACF = []\n",
    "target_checkin = []\n",
    "for batch in valid_loader:\n",
    "    pred_IACF.append(model(batch)['IACF'].detach().cpu().numpy())\n",
    "    target_checkin.append(batch['checkin'].numpy())\n",
    "pred_IACF = np.concatenate(pred_IACF, axis=0)\n",
    "target_checkin = np.concatenate(target_checkin, axis=0)\n",
    "\n",
    "CORR = get_CORR_numpy2(pred_IACF, target_checkin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
